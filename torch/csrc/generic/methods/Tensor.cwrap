// TODO: check that there are no args
[[
  name: THPTensor_(elementSize)
  python_name: element_size
  cpu_half: True
  auto_gpu: False
  only_register: True
]]
static PyObject * THPTensor_(elementSize)(THPTensor *self, PyObject *args)
{
  return PyLong_FromLong(THStorage_(elementSize)(LIBRARY_STATE_NOARGS));
}

// TODO: check that there are no args
[[
  name: THPTensor_(storage)
  python_name: storage
  cpu_half: True
  auto_gpu: False
  only_register: True
]]
static PyObject * THPTensor_(storage)(THPTensor *self, PyObject *args)
{
  // TODO: memory leak on error
  THStorage *result = THTensor_(storage)(LIBRARY_STATE self->cdata);
  if (result == NULL)
    Py_RETURN_NONE;
  THStorage_(retain)(LIBRARY_STATE result);
  THStoragePtr _tmp = result;
  PyObject *ret = THPStorage_(New)(result);
  _tmp.release();
  return ret;
}

[[
  name: storageOffset
  python_name: storage_offset
  cpu_half: True
  auto_gpu: False
  return: long
  arguments:
    - THTensor* self
]]

[[
  name: nDimension
  python_name: ndimension
  cpu_half: True
  auto_gpu: False
  return: long
  arguments:
    - THTensor* self
]]
[[
  name: THPTensor_(nDimension)
  python_name: dim
  cpu_half: True
  auto_gpu: False
  only_register: True
  method_flags: METH_KEYWORDS
]]

[[
  python_name: index
  name: THPTensor_(getValue)<true>
  only_register: True
  override_method_flags: METH_O
]]

[[
  python_name: _set_index
  name: THPTensor_(setIndex)
  only_register: True
]]
PyObject * THPTensor_(setIndex)(THPTensor *self, PyObject *args)
{
  THPUtils_assert(PyTuple_GET_SIZE(args) == 2, "set_index takes exactly two "
      "arguments (%d given)", (int)PyTuple_GET_SIZE(args));
  if (THPTensor_(setValue)<true>(self, PyTuple_GET_ITEM(args, 0), PyTuple_GET_ITEM(args, 1)) != 0)
    return NULL;
  Py_RETURN_NONE;
}

[[
  name: resize_
  return: self
  cname: resize
  cpu_half: True
  arguments:
    - THTensor* self
    - arg: THSize* size
      long_args: True
    - CONSTANT NULL
]]

[[
  name: zeros
  only_stateless: True
  auto_gpu: False
  return: argument 0
  arguments:
    - arg: THTensor* result
      output: True
    - arg: THSize* size
      long_args: True
]]

[[
  name: ones
  only_stateless: True
  auto_gpu: False
  return: argument 0
  arguments:
    - arg: THTensor* result
      output: True
    - arg: THSize* size
      long_args: True
]]

[[
  name: numel
  return: long
  cname: nElement
  cpu_half: True
  auto_gpu: False
  with_stateless: True
  arguments:
    - THTensor* self
]]
[[
  name: THPTensor_(numel)
  python_name: nelement
  cpu_half: True
  auto_gpu: False
  only_register: True
  method_flags: METH_KEYWORDS
]]

[[
  name: set_
  cname: set
  cpu_half: True
  auto_gpu: False
  return: argument 0
  options:
    - cname: set
      arguments:
        - THTensor* self
        - THTensor* source
    - cname: setStorage
      arguments:
        - THTensor* self
        - CONSTANT NULL, 0, NULL, NULL
    - cname: setStorage
      before_call: THLongStoragePtr __storage_size = THLongStorage_newWithSize1(THStorage_(size)(LIBRARY_STATE arg_storage));
      arguments:
        - THTensor* self
        - THStorage* storage
        - CONSTANT 0
        - CONSTANT __storage_size.get()
        - CONSTANT NULL
    - cname: setStorage
      arguments:
        - THTensor* self
        - THStorage* sourceStorage
        - long storage_offset
        - arg: THSize* size
          long_args: True
        - CONSTANT NULL
    - cname: setStorage
      arguments:
        - THTensor* self
        - THStorage* sourceStorage
        - long storage_offset
        - THSize* size
        - THStride* strides
]]

[[
  name: THPTensor_(select)
  python_name: select
  cpu_half: True
  auto_gpu: False
  only_register: True
]]
static PyObject * THPTensor_(select)(THPTensor *self, PyObject *args)
{
  HANDLE_TH_ERRORS
  long dim, idx;
  if (!PyArg_ParseTuple(args, "ll", &dim, &idx))
    return NULL;

  int ndim = THTensor_(nDimension)(LIBRARY_STATE self->cdata);

  THPUtils_assert(dim >= -(ndim) && dim < (ndim),
    "dimension out of range (expected to be in range of [%d, %d], but got %d)",
    -(ndim), (ndim)-1, dim);
  if (dim<0) dim += ndim;

  if(ndim > 1) {
    THTensorPtr selected = THTensor_(newWithTensor)(LIBRARY_STATE self->cdata);
    THTensor_(select)(LIBRARY_STATE selected.get(), NULL, dim, idx);
    return THPTensor_(New)(selected.release());
  }
  else {
    THArgCheck(ndim == 1, 1, "empty Tensor");
    return THPUtils_(newReal)(THTensor_(get1d)(LIBRARY_STATE self->cdata, idx));
  }
  END_HANDLE_TH_ERRORS
}

PyObject * THPTensor_(size)(PyObject *self, PyObject *args, PyObject *kwargs)
{
  HANDLE_TH_ERRORS
  THTensor* tensor = ((THPTensor*)self)->cdata;
  if (PyTuple_Size(args) == 0 && (!kwargs || PyDict_Size(kwargs) == 0)) {
    return THPSize_New(tensor->nDimension, tensor->size);
  }

  int tuplecount = args ? PyTuple_Size(args) : 0;
  int dictcount = kwargs ? PyDict_Size(kwargs) : 0;

  PyObject* pydim = NULL;
  if (tuplecount == 1 && dictcount == 0) {
    pydim = PyTuple_GET_ITEM(args, 0);
  } else if (dictcount == 1 && tuplecount == 0) {
    pydim = PyDict_GetItemString(kwargs, "dim");
  }

  if (pydim && THPUtils_checkLong(pydim)) {
    int dim = (int)THPUtils_unpackLong(pydim);
    if (dim < 0)
      dim += tensor->nDimension;
    return PyInt_FromLong(THTensor_(size)(LIBRARY_STATE tensor, dim));
  }

  THPUtils_invalidArguments(args, kwargs, "size", 2, "(int dim)", "no arguments");
  return NULL;
  END_HANDLE_TH_ERRORS
}
[[
  name: THPTensor_(size)
  python_name: size
  cpu_half: True
  auto_gpu: False
  method_flags: METH_KEYWORDS
  only_register: True
]]

PyObject * THPTensor_(stride)(PyObject *self, PyObject *args, PyObject *kwargs)
{
  HANDLE_TH_ERRORS
  THTensor* tensor = ((THPTensor*)self)->cdata;
  if (PyTuple_Size(args) == 0 && (!kwargs || PyDict_Size(kwargs) == 0)) {
    PyObject* stride = PyTuple_New(tensor->nDimension);
    for (int i = 0; i != tensor->nDimension; ++i) {
      PyTuple_SET_ITEM(stride, i, PyLong_FromLong(tensor->stride[i]));
    }
    return stride;
  }

  int tuplecount = args ? PyTuple_Size(args) : 0;
  int dictcount = kwargs ? PyDict_Size(kwargs) : 0;

  PyObject* pydim = NULL;
  if (tuplecount == 1 && dictcount == 0) {
    pydim = PyTuple_GET_ITEM(args, 0);
  } else if (dictcount == 1 && tuplecount == 0) {
    pydim = PyDict_GetItemString(kwargs, "dim");
  }

  if (pydim && THPUtils_checkLong(pydim)) {
    int dim = (int)THPUtils_unpackLong(pydim);
    if (dim < 0)
      dim += tensor->nDimension;
    return PyInt_FromLong(THTensor_(stride)(LIBRARY_STATE tensor, dim));
  }

  THPUtils_invalidArguments(args, kwargs, "stride", 2, "(int dim)", "no arguments");
  return NULL;
  END_HANDLE_TH_ERRORS
}
[[
  name: THPTensor_(stride)
  python_name: stride
  cpu_half: True
  auto_gpu: False
  method_flags: METH_KEYWORDS
  only_register: True
]]

[[
  name: fill_
  cname: fill
  return: self
  arguments:
    - THTensor* self
    - real value
]]

[[
  name: isSameSizeAs
  python_name: is_same_size
  cpu_half: True
  auto_gpu: False
  return: bool
  arguments:
    - THTensor* self
    - THTensor* other
]]

[[
  name: isContiguous
  python_name: is_contiguous
  cpu_half: True
  auto_gpu: False
  return: bool
  arguments:
    - THTensor* self
]]

[[
  name: isSetTo
  python_name: is_set_to
  cpu_half: True
  auto_gpu: False
  return: bool
  arguments:
    - THTensor* self
    - THTensor* tensor
]]

[[
  name: maskedFill_
  cname: maskedFill
  python_name: masked_fill_
  return: self
  arguments:
    - THTensor* self
    - THBoolTensor* mask
    - real value
]]

[[
  name: maskedCopy_
  cname: maskedCopy
  python_name: masked_copy_
  return: self
  arguments:
    - THTensor* self
    - THBoolTensor* mask
    - THTensor* source
]]

[[
  name: maskedSelect
  python_name: masked_select
  with_stateless: True
  return: argument 0
  arguments:
    - arg: THTensor* result
      output: True
    - THTensor* self
    - THBoolTensor* mask
]]

[[
  name: transpose
  with_stateless: True
  cname: newTranspose
  cpu_half: True
  auto_gpu: False
  return: THTensor*
  arguments:
    - THTensor* self
    - arg: long dim0
      wrap_dim: self
    - arg: long dim1
      wrap_dim: self
]]

[[
  name: transpose_
  cname: transpose
  cpu_half: True
  auto_gpu: False
  return: self
  arguments:
    - THTensor* self
    - THTensor* self
    - arg: long dim0
      wrap_dim: self
    - arg: long dim1
      wrap_dim: self
]]

[[
  name: t
  with_stateless: True
  auto_gpu: False
  cname: newTranspose
  return: THTensor*
  before_call: |
    long ndim = arg_self->nDimension;
    THPUtils_assert(ndim == 2, "t() expects a 2D tensor, but self is %ldD", ndim);
  arguments:
    - THTensor* self
    - CONSTANT 0
    - CONSTANT 1
]]

[[
  name: t_
  cname: transpose
  auto_gpu: False
  return: self
  before_call: |
    long ndim = arg_self->nDimension;
    THPUtils_assert(ndim == 2, "t_() expects a 2D tensor, but self is %ldD", ndim);
  arguments:
    - THTensor* self
    - THTensor* self
    - CONSTANT 0
    - CONSTANT 1
]]

[[
  name: squeeze
  cpu_half: True
  with_stateless: True
  return: argument 0
  options:
    - arguments:
        - arg: THTensor* result
          output: True
        - THTensor* self
    - cname: squeeze1d
      arguments:
        - arg: THTensor* result
          output: True
        - THTensor* self
        - arg: long dim
          wrap_dim: self
]]

[[
  name: squeeze_
  cpu_half: True
  return: self
  options:
    - cname: squeeze
      arguments:
        - THTensor* self
        - THTensor* self
    - cname: squeeze1d
      arguments:
        - THTensor* self
        - THTensor* self
        - arg: long dim
          wrap_dim: self
]]

[[
  name: unsqueeze
  with_stateless: True
  cpu_half: True
  auto_gpu: False
  return: argument 0
  cname: unsqueeze1d
  arguments:
    - arg: THTensor* result
      output: True
    - THTensor* self
    - arg: long dim
      wrap_dim: self+1
]]

[[
  name: unsqueeze_
  cpu_half: True
  auto_gpu: False
  return: self
  cname: unsqueeze1d
  arguments:
    - THTensor* self
    - THTensor* self
    - arg: long dim
      wrap_dim: self+1
]]

[[
  name: nonzero
  with_stateless: True
  return: argument 0
  arguments:
    - arg: THIndexTensor* result
      output: True
    - THTensor* self
]]

[[
  name: contiguous
  cname: newContiguous
  return: THTensor*
  arguments:
    - THTensor* self
]]

[[
  name: clone
  cname: newClone
  return: THTensor*
  arguments:
    - THTensor* self
]]

[[
  name: view
  cname: newView
  auto_gpu: False
  return: THTensor*
  arguments:
    - THTensor* self
    - arg: THSize* size
      long_args: True
]]

[[
  name: resizeAs_
  python_name: resize_as_
  cname: resizeAs
  return: self
  arguments:
    - THTensor* self
    - THTensor* template
]]

[[
  name: indexSelect
  python_name: index_select
  with_stateless: True
  return: argument 0
  arguments:
    - arg: THTensor* result
      output: True
    - THTensor* self
    - arg: long dim
      wrap_dim: self
    - THIndexTensor* index
]]

[[
  name: indexCopy_
  python_name: index_copy_
  cname: indexCopy
  return: argument 0
  arguments:
    - THTensor* self
    - arg: long dim
      wrap_dim: self
    - THIndexTensor* index
    - THTensor* source
]]

[[
  name: indexAdd_
  python_name: index_add_
  cname: indexAdd
  return: argument 0
  arguments:
    - THTensor* self
    - arg: long dim
      wrap_dim: self
    - THIndexTensor* index
    - THTensor* source
]]

[[
  name: indexFill_
  python_name: index_fill_
  cname: indexFill
  return: argument 0
  arguments:
    - THTensor* self
    - arg: long dim
      wrap_dim: self
    - THIndexTensor* index
    - real value
]]

[[
  name: narrow
  cpu_half: True
  auto_gpu: False
  return: argument 0
  arguments:
    - arg: THTensor* result
      output: True
    - THTensor* self
    - arg: long dimension
      wrap_dim: self
    - long start
    - long length
]]

[[
  name: unfold
  cpu_half: True
  auto_gpu: False
  return: argument 0
  arguments:
    - arg: THTensor* result
      output: True
    - THTensor* self
    - arg: long dimension
      wrap_dim: self
    - long size
    - long step
]]

[[
  name: range
  only_stateless: True
  defined_if: "!IS_CUDA"
  return: argument 0
  before_arg_assign: |
    PyErr_WarnEx(PyExc_UserWarning, "torch.range is deprecated in favor of torch.arange "
        "and will be removed in 0.3. Note that arange generates values in [start; end), "
        "not [start; end].", 1);
  arguments:
    - arg: THTensor* result
      output: True
    - accreal start
    - accreal end
    - arg: accreal step
      default: 1
]]

[[
  name: arange
  cname: range
  only_stateless: True
  defined_if: "!IS_CUDA"
  return: argument 0
  options:
    - before_call: |
        if (mod_traits<accreal>::mod(arg_end - arg_start, arg_step) == 0) {
          arg_end -= arg_step;
        }
      arguments:
        - arg: THTensor* result
          output: True
        - accreal start
        - accreal end
        - accreal step
    - before_call: |
        if (mod_traits<accreal>::mod(arg_end - arg_start, 1) == 0) {
          arg_end -= 1;
        }
      arguments:
        - arg: THTensor* result
          output: True
        - accreal start
        - accreal end
        - CONSTANT 1
]]

[[
  name: scatter_
  return: argument 0
  options:
    - cname: scatter
      arguments:
        - THTensor* self
        - arg: long dim
          wrap_dim: self
        - THIndexTensor* index
        - THTensor* src
    - cname: scatterFill
      arguments:
        - THTensor* self
        - arg: long dim
          wrap_dim: self
        - THIndexTensor* index
        - real value
]]

[[
  name: gather
  with_stateless: True
  return: argument 0
  before_call: |
    THLongStoragePtr _size = THIndexTensor_(newSizeOf)(LIBRARY_STATE arg_index);
    THTensor_(resize)(LIBRARY_STATE arg_result, _size, NULL);
  arguments:
    - arg: THTensor* result
      output: True
    - THTensor* self
    - arg: long dim
      wrap_dim: self
    - THIndexTensor* index
]]

[[
  name: THPTensor_stateless_(cat)
  python_name: cat
  method_flags: METH_KEYWORDS
  only_register: True
  only_stateless: True
]]
#ifndef TH_REAL_IS_HALF
static PyObject * THPTensor_stateless_(cat)(THPTensor *_unused, PyObject *args, PyObject *kwargs)
{
  HANDLE_TH_ERRORS
#if IS_CUDA
  THCPAutoGPU __autogpu_guard(-1);
#endif
  Py_ssize_t _tuplecount = args ? PyTuple_GET_SIZE(args) : 0;
  Py_ssize_t _dictcount = kwargs ? PyDict_Size(kwargs) : 0;
  Py_ssize_t _argcount = _tuplecount + _dictcount;
  std::vector<THPObjectPtr> items;
  std::vector<THTensor *> item_tensors;
  Py_ssize_t seq_length;
  THPTensorPtr result;
  long dimension = -1;
  PyObject *sequence = _tuplecount > 0 ? PyTuple_GET_ITEM(args, 0)
                     : (kwargs ? PyDict_GetItemString(kwargs, "seq") : NULL);
  PyObject *pydim = _tuplecount > 1 ? PyTuple_GET_ITEM(args, 1)
                  : (kwargs ? PyDict_GetItemString(kwargs, "dim") : NULL);

  if (_argcount == 0 || _argcount > 2)
    goto invalid_arguments;
  if (sequence == NULL || (_argcount > 1 && pydim == NULL))
    goto invalid_arguments;
  if (!PySequence_Check(sequence))
    goto invalid_arguments;
  seq_length = PySequence_Length(sequence);
  THPUtils_assert(seq_length > 0, "sequence length has to be positive, but is %lld",
                  (long long)seq_length);

  items.reserve(seq_length);
  item_tensors.reserve(seq_length);
  for (int i = 0; i < seq_length; i++) {
    items.push_back(PySequence_ITEM(sequence, i));
    if (!THPTensor_(Check)(items[i].get()))
      goto invalid_arguments;
    item_tensors.push_back(((THPTensor*)items[i].get())->cdata);
  }

  if (pydim) {
    if (!THPUtils_checkLong(pydim))
      goto invalid_arguments;
    dimension = THPUtils_unpackLong(pydim);
  } else {
    dimension = 0;
  }

  for (THTensor *t : item_tensors) {
    auto ndim = THTensor_(nDimension)(LIBRARY_STATE t);
    if (ndim > 0) {
      THPUtils_assert(dimension > 0 ? dimension < ndim : ndim + dimension >= 0,
                      "dimension out of range - got %d but the tensor is only %dD",
                      dimension, ndim);
      if (dimension < 0) dimension += ndim;
      break;
    }
  }

#if IS_CUDA
  __autogpu_guard.setDevice(THTensor_(getDevice)(LIBRARY_STATE item_tensors[0]));
#endif

  result = (THPTensor *)THPTensor_(NewEmpty)();
  if (!result) return NULL;

  THTensor_(catArray)(LIBRARY_STATE result->cdata, item_tensors.data(),
      items.size(), dimension);
  return (PyObject*)result.release();

invalid_arguments:
  THPUtils_invalidArguments(args, kwargs, "cat", 2,
      "(sequence[" THPTensorStr "] seq)",
      "(sequence[" THPTensorStr "] seq, int dim)");
  return NULL;
  END_HANDLE_TH_ERRORS
}
#endif

[[
  name: data_ptr
  return: void*
  cpu_half: True
  cname: data
  arguments:
    - THTensor* self
]]

[[
  name: equal
  with_stateless: True
  return: bool
  arguments:
    - THTensor* self
    - THTensor* other
]]

[[
  python_name: copy_
  name: THPTensor_(copy_)
  cpu_half: True
  method_flags: METH_KEYWORDS
  only_register: True
]]
PyObject * THPTensor_(copy_)(PyObject *self, PyObject *args, PyObject *kwargs)
{
  HANDLE_TH_ERRORS
  return THPCopyMethod(THTensor_(copy_functions), self, args, kwargs);
  END_HANDLE_TH_ERRORS
}
